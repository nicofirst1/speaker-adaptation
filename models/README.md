# Models

This directory contains the model definitions and scripts to train, evaluate and compare the models.

You can download the pretrained models from [this link](https://uva.data.surfsara.nl/index.php/s/wlnq5rhG3VxFqjb).

To preprocess the data for the Ref and ReRef models, you should run speaker/utils/new_dataset_processor.py. For the Copy model, please run speaker/utils/new_dataset_processor_copy.py.

To preprocess the data for the listener model, please run listener/utils/new_dataset_processor_BERT.py.

# Generation models (speaker):
We use a different naming convention in the repository as compared to paper:

- Ref - model_speaker_base.py
- ReRef - model_speaker_hist_att.py
- Copy - model_speaker_hist_att_COPY.py

*(The baseline version of ReRef uses the first generated utterance in all later mentions. It is not a trained model, as it is simply copying the first generated outputs from the ReRef model. You can find its implementation in baseline_from_generated1st_REREF.py.)*

In speaker/utils/new_dataset_processor.py, we provide the code for processing the utterance-based reference chains for Ref and ReRef generation models. For the Copy model, however, we use speaker/utils/new_dataset_processor_copy.py, as it needs to keep track of the original forms of <unk> tokens. 

To train the Ref model, an example command would be:

``python train_speaker_generic.py -dropout 0.3 -metric bert -batch_size 32 -model_type base -reduction sum -subset_size -1 -seed 1 -learning_rate 0.0001 -beam_size 3 -embedding_dim 1024 -shuffle``

To train the ReRef model, an example command would be:

``python train_speaker_generic.py -dropout 0.3 -metric bert -batch_size 16 -model_type hist_att -reduction sum -subset_size -1 -seed 42 -learning_rate 0.0001 -beam_size 3 -embedding_dim 1024 -shuffle``

To train the Copy model, an example command would be:

``python train_speaker_generic_COPY.py -metric bert -batch_size 32 -model_type copy -reduction sum -subset_size -1 -seed 1 -learning_rate 0.0001 -beam_size 3 -embedding_dim 1024 -shuffle``

Generated outputs from the 5 runs of models along with their contexts for the test sets are obtained in compare_ref_hyp_* scripts and provided in:

- generated_outputs_model_all_base_TEST.txt
- generated_outputs_model_all_copy_TEST.txt
- generated_outputs_model_all_histatt_TEST.txt

pretrained_speaker* scripts load trained generation models and evaluate them on the test set.

# Reference resolution models (listener):
We use a different naming convention in the repository as compared to paper:

- One-hot baseline - model_bert_BASELINE_1H.py
- Ablation - model_bert_att_ctx.py
- Proposed - model_bert_att_ctx_hist.py

In **listener/utils/new_dataset_processor_BERT.py**, we provide the code for processing the utterance-based reference chains (getting the image contexts, target image and linguistic context). Here, we also convert the linguistic input into BERT representations.

To train the proposed model, an example command would be:

``python train_listener_bert_CE_generic.py -dropout 0.5 -batch_size 32 -bert_type base -model_type bert_att_ctx_hist -reduction sum -subset_size -1 -seed 3 -learning_rate 0.0001 -shuffle``

To train the ablated model, an example command would be:

``python train_listener_bert_CE_generic.py -dropout 0.5 -batch_size 32 -bert_type base -model_type bert_att_ctx -reduction sum -subset_size -1 -seed 3 -learning_rate 0.0001 -shuffle``

To train the one-hot baseline, an example command would be:

``python train_listener_bert_CE_generic_BASELINES_1H.py -batch_size 32 -bert_type base -model_type baseline_1H -reduction sum -subset_size -1 -seed 3 -learning_rate 0.0001 -shuffle``


eval_* scripts run evaluations of various pretrained models depending on whether we are looking at only the first utterances or the cases where the target image already has conversational history.


# Inputting generation model outputs into reference resolution models
predict_listener* scripts take in trained generation models and plug in their generations into the best reference resolution model. In this way, we investigate how discriminative the utterances generated by the generation models are. '_BREAKDOWN' versions look at first and later utterances separately.
 

