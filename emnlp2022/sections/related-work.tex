% !TEX root = ../main.tex

%==========================================
\section{Related Work}
\label{sec:related-work}
%==========================================

\note{max 2 columns}\\
\note{Ece + Mario}

% \mar{I am not so sure about the structure of this section. This is a draft to get started.}\san{I wonder if we really need the two subsections on Reference games and REG here. My two cents are that all this could be introduced and discussed in the Intro, where we place our work into a theoretical/computational framework. Instead, the two subsections on adaptation and ToM are crucial here (this is actual Related Work)}

% In this paper, we use reference games as a testbed for the adaptation of a proficient speaker to listeners with limited semantic knowledge.
% Speaker adaptation can be studied from a variety of perspectives and tackled with different technical solutions. We first present reference games in §\ref{sec:background-reference-games} and computational models of reference in §\ref{sec:background-reg}; then, we discuss different approaches to speaker adaptation in §\ref{sec:background-speaker-adaptation}.

In this section, we first survey the types of conversational settings that were addressed in previous work on speaker adaptation (§\ref{sec:relwork-scenarios}) and then we discuss common approaches to modelling speaker adaptation in such scenarios (§\ref{sec:relwork-modelling}).
% \noindent\textbf{Referring in context / Repeated reference games}
% \ece{move citations from intro under this title, clark, garrod etc.}
% \begin{itemize}
% \item Subsequent references \cite{gupta2005automatic,jordan2005learning,stoia-etal-2006-noun,viethen-etal-2011-generating}
% \item Dialogue history \cite{brockmann2005modelling,buschmeier-etal-2009-alignment,stoyanchev-stent-2009-lexical,lopes2015rule,hu2016entrainment,dusek-jurcicek-2016-context}
% \end{itemize}

\raq{I'm not convinced by the structure of this section.}

\subsection{Speaker Adaptation Scenarios}
\label{sec:relwork-scenarios}
\raq{This subsection seems more appropriate for the introduction?}
REG systems can be conceptualised as \textit{speaker} agents that produce referring expressions to be resolved in context by \textit{listener} agents. For such interactions to be successful, the speaker must form a mental representation of the listener's comprehension abilities. This ability is often referred to as Theory of Mind \cite[ToM;][]{premack1978tom} and it is considered as a fundamental social-cognitive skill for language acquisition and language use \cite{tomasello2005constructing}. 
Symmetry is a standard assumption in REG, with speaker and listener sharing the same general semantic knowledge \cite{mao2016generation,Yu2017AJS}. However, in real life, the listener's knowledge may not be as complete as that of the speaker, as it is the case in adult-child and teacher-learner interactions \mar{add refs}, 
\raq{the next set of citations are not about ``real life'', so it is a bit odd to list them here as if these were `real life' scenarios, plus there is unnecessary repetition because we need to elaborate on what is actually done in these works and this is done in the next section}
as well as in interactions where the listener has impaired perceptual systems \cite{corona2019modeling}, limited semantic knowledge \citep{bao-etal-2022-learning}, limited information about the state of affairs \citep{bara-etal-2021-mindcraft}, or limited language proficiency \citep{zhu2021few}. Our experiments also feature an asymmetric setup, one in which the speaker is proficient---i.e., it is an REG model with knowledge of all semantic domains---and the listeners are domain-specific reference resolution models with restricted semantic knowledge.

\subsection{Modelling Speaker Adaptation}
\label{sec:relwork-modelling}
The most commonly adopted framework for modelling ToM in language use is the Rational Speech Act framework \cite{goodman2013knowledge,goodman2016pragmatic}. RSA is typically used to model pragmatic, listener-aware REG in symmetric conversational settings \cite{andreas-klein-2016-reasoning,monroe-etal-2017-colors,cohn-gordon-etal-2018-pragmatically} but there have also been attempts at modelling asymmetric interactions. For example, \citet{bao-etal-2022-learning} reweigh the probability of candidate referring utterances by their expected utility, i.e., their likelihood to be successfully interpreted by a listener with comprehension deficiencies. Generating and ranking multiple utterances, however, is an inefficient and cognitively implausible production mechanism. For these reasons, others have tried to condition the speaker model prior to utterance generation. Adaptation can be achieved through implicit conditioning of the speaker's statistical model via recursive pragmatic reasoning \cite{hawkins2020continual} as well as through external modules---e.g., policy networks conditioned on listener agent embeddings \cite{corona2019modeling} and mental state prediction networks \cite{bara-etal-2021-mindcraft,zhu2021few}. A clear advantage of the first approach is its simpler model architecture. On the other hand, relying on multiple modules allows for continual updates that do not result in catastrophic forgetting for the main speaker model. Our proposed model can been as a hybrid that borrows from both directions. We minimise architecture complexity by using a simple ToM module; thanks to this module, our speaker can continuously adapt to different listeners while maintaining its original proficiency. We are inspired by the plug-and-play approach to controllable text generation \cite{dathathri2020plug}, which has been used to steer large pre-trained language models towards generating texts with certain attributes (e.g., positive or negative sentiment) while keeping the model parameters unchanged.

\ece{mention concurrent work? https://arxiv.org/pdf/2206.08349.pdf}

\textbf{Methods for language model adaptation}
To adapt or control the generated outputs of language models, various methods have been proposed such as prefix-tuning \citep{prefix}, prompting \citep{gpt3}, adapters \citep{houlsby19a, madg}, and plug-and-play updates \citep{dathathri2020plug}.

\san{As mentioned above, I'd keep just the two subsections above in the RW. }

% the adaptation/learning approach by Hawkins at al CoNLL-2020 (and any other approaches that may be similar), its limitations
% How our approach differs from all the above — here mention that we are inspired by PPLM, what PPLM has been used for up to now, and how we adapt the idea to model ToM in a referring task with asymmetric knowledge.

% \citet{corona2019modeling} have proposed a machine learning model that forms an internal  representation of other agents that encodes how well they would understand different referring utterances presented to them. They use an asymmetric setup where a proficient speaker learns to adapt to a population of listeners with a different understanding of visual attributes. They show that a mental model over other agents’ understanding of visual attributes makes the interactions more successful.

% corona -- We extend this work by using an image reference game that elicits complex referring expressions---drawing from a full vocabulary rather than from a small subset thereof and without severe length constraints---and by creating a population of listeners that differ according to their general semantic knowledge---as represented by a high-dimensional semantic space---rather than to a set of predefined attributes.

% Bao -- not truly plug and play and ranking of candidates
% Bara -- complex modular system, 


% In , but (typically) no asymmetric world/semantic knowledgeIn this paper, ...


% \noindent\textbf{RSA / TOM - Pragmatic generation}

% \cite{premack1978tom,corona2019modeling,andreas-klein-2016-reasoning,vedantam2017context,cohn-gordon-etal-2018-pragmatically}

% Reference-Centric Models for Grounded Collaborative Dialogue
% Daniel Fried, Justin T. Chiu, Dan Klein

% MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks

% nematzadeh on ToM?

% Zeynep's paper

% Will Monroe, Robert X.D. Hawkins, Noah D. Goodman, and Christopher Potts. 2017. Colors in context:
% A pragmatic neural model for grounded language
% understanding

% Hawkins

% \subsection{Adaptive Generation and Multi-Agent Communication}
% \label{sec:background-generation-multiagent}

% \noindent\textbf{Adaptive generation} adaptation / domain adaptation

% Controlled generation \cite{nguyen2017plug,dathathri2020plug,keskar2019ctrl,ziegler2019finetuning}
% PPLM paper


% Domain adaptation (in captioning, dialogue, QA etc.)

% Some mention of prompting, prefix tuning and adapters (+transformers)
% Diffusion-LM