% !TEX root = ../main.tex

%==========================================
\section{Analysis}
\label{sec:analysis}
%==========================================
Possible quantitative analyses / evaluation
\begin{itemize}
\item \textbf{Task-level }evaluation: does the listeners' reference resolution become more accurate as a result of speaker adaptation? We compare listener accuracy and Mean Reciprocal Rank (MRR) in response to adapted utterances with that obtained for utterances generated by a static model \cite{takmaz-etal-2020-refer}.
\item \textbf{Utterance-level}
	\begin{itemize}
	\item We compute several metrics that are commonly used for Natural Language Generation. We consider three measures based on \emph{n-}gram matching---BLEU-2~\cite{Papineni:2002},
%\footnote{BLEU-2, which is based on bigrams, appears to be more informative than BLEU with longer $n$-grams in dialogue response generation \cite{liu-etal-2016-evaluate}}
ROUGE~\cite{Lin2004}, and CIDEr~\cite{cider}---as well as BERTScore F1~\cite{bert-score}, which instead of exact string matches relies on the semantic similarity between tokens. With these measures, we capture the degree of similarity between generated referring utterances and their human counterparts. We expect adapted utterances to be less similar to the human references due to more partner-specific language use (not just transient conceptual pacts but coordination of the entire semantic space).
automatic gen metrics
	\item We also measure the length of the generated utterances to check if the adaptive model reproduces the reduction trend found in humans \cite{haber2019photobook,hawkins2020continual} and in history-aware generation models \cite{takmaz-etal-2020-refer,hawkins2020characterizing}.
	\item The speaker's adaptation may lead to language that is highly effective for the purposes of the image reference task but which does not sound natural, human-like (e.g., repetitions, "heavily" ungrammatical sentences). We will compare the distribution of POS sequences produced by the models with those produced by humans (e.g., we know that Noun Noun sequences become more frequent as we move along a reference chain). We will also use chunking to identify idiosyncratic expressions used by the models and/or by humans.
	\end{itemize}
\item \textbf{Vocabulary-level}
	\begin{itemize}
	\item We will look at the POS distribution over the vocabulary produced by the models and by humans over the entire PhotoBook dataset, as a function of adaptation and of utterance position in the chain.
	\item A possible adaptation strategy for speaker is to reduce its vocabulary to a few very effective words. We check if this is the case by monitoring the vocabulary size and type-token ratio of the models (adaptive and static) and comparing it to humans'.
	\item Another way to assessing how human-like the models' vocabularies are is to check if they follow the Zipfian distribution that we expect from a natural language. 
	\end{itemize}
\item \textbf{Model-level}: We also analyse the effect that adaptation has on the model's internal representations and on its lexical choices.
	\begin{itemize}
	\item What type of internal representations is it more useful to modify? Does adaptation at different representational levels produce different kinds of linguistic output?
	\item How much should the model's hidden representations be modified to strike a good balance between general fluency and partner-specificity? 
	\item How do the model's vocabulary distributions change as a result of adaptation? Do they become less (more) uniform? Do certain successful vocabulary items become more prominent? Are there certain timesteps where the change is stronger?
	\item What effect does adaptation have on the decision making of the model? We can look at the generation samples obtained for adaptive and static models and monitor their overlap as a function of the speed and "depth" of adaptation.
	\end{itemize}
\end{itemize}

Possible qualitative analysis
\begin{itemize}
\item \textbf{Overextension}. Three main types of semantic relations connect conventional and overextended referents of a word \cite{rescorla1980}:
	\begin{itemize}
	\item categorical: linking objects that are close in a taxonomy (e.g., bed referring to a sofa)
	\item analogical: linking objects with shared perceptual properties (e.g., giraffe referring to a lamp)
	\item predicate-based: linking objects that co-occur frequently (e.g., wheels referring to a bike)
	\end{itemize}
Analogical relations are those that can most be exploited by the speaker given a static listener model. Good visual representations should allow the model to find abstract similarities between taxonomically distant and rarely co-occurring objects. 

\end{itemize}