{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd speaker_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASED ON speaker_old/pretrained_speaker_TEST.py\n",
    "# that checks the trained ReRef generation models on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from models.model_speaker_base import SpeakerModelBase\n",
    "from models.model_speaker_hist_att import SpeakerModelHistAtt\n",
    "\n",
    "from utils.SpeakerDataset import SpeakerDataset\n",
    "from utils.Vocab import Vocab\n",
    "\n",
    "# from evals import eval_beam_base, eval_beam_histatt\n",
    "# !!! FOR DEMO PURPOSES, WE ARE USING THE METHOD eval_beam_histatt_DEMO given in this notebook\n",
    "\n",
    "from nlgeval import NLGEval\n",
    "\n",
    "import os\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_attn(actual_num_tokens, max_num_tokens, device):\n",
    "\n",
    "    masks = []\n",
    "\n",
    "    for n in range(len(actual_num_tokens)):\n",
    "\n",
    "        # items to be masked are TRUE\n",
    "        mask = [False] * actual_num_tokens[n] + [True] * (max_num_tokens - actual_num_tokens[n])\n",
    "\n",
    "        masks.append(mask)\n",
    "\n",
    "    masks = torch.tensor(masks).unsqueeze(2).to(device)\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.6.0 available.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'BertConfig' has no attribute 'pretrained_config_archive_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9030dec76b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m def eval_beam_histatt_DEMO(split_data_loader, model, args, best_score, print_gen, device,\n",
      "\u001b[0;32m~/anaconda3/envs/uva/lib/python3.8/site-packages/bert_score/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0.3.2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uva/lib/python3.8/site-packages/bert_score/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_config_archive_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m               \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXLNetConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_config_archive_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m               \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRobertaConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_config_archive_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'BertConfig' has no attribute 'pretrained_config_archive_map'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "\n",
    "from bert_score import score\n",
    "\n",
    "def eval_beam_histatt_DEMO(split_data_loader, model, args, best_score, print_gen, device,\n",
    "                      beam_size, max_len, vocab, mask_attn, nlgeval_obj, isValidation, timestamp, isTest):\n",
    "    \"\"\"\n",
    "        Evaluation\n",
    "\n",
    "        :param beam_size: beam size at which to generate captions for evaluation\n",
    "        :return: Official MSCOCO evaluator scores - bleu4, cider, rouge, meteor\n",
    "        \"\"\"\n",
    "\n",
    "    # Lists to store references (true captions), and hypothesis (prediction) for each image\n",
    "    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
    "    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
    "\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    empty_count = 0\n",
    "\n",
    "    breaking = args.breaking\n",
    "\n",
    "    sos_token = torch.tensor(vocab['<sos>']).to(device)\n",
    "    eos_token = torch.tensor(vocab['<eos>']).to(device)\n",
    "\n",
    "    if isValidation:\n",
    "        split = 'val'\n",
    "    elif isTest:\n",
    "        split = 'test'\n",
    "    else:\n",
    "        split = 'train'\n",
    "\n",
    "    file_name = args.model_type + '_' + args.metric + '_' + split + '_' + timestamp # overwrites previous versions!\n",
    "\n",
    "    for i, data in enumerate(split_data_loader):\n",
    "        # print(i)\n",
    "\n",
    "        completed_sentences = []\n",
    "        completed_scores = []\n",
    "\n",
    "        beam_k = beam_size\n",
    "\n",
    "        if breaking and count == 5:\n",
    "            break\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        # dataset details\n",
    "        # only the parts I will use for this type of model\n",
    "\n",
    "        utterance = data['utterance']  # to be decoded, we don't use this here in beam search!\n",
    "        #target_utterance = utterance[:,1:]\n",
    "        # I am using the one below as references for the calculation of metric scores\n",
    "        orig_text_reference = data['orig_utterance'] # original reference without unk, eos, sos, pad\n",
    "        reference_chain = data['reference_chain'][0] #batch size 1  # full set of references for a single instance\n",
    "        # obtained from the whole chain\n",
    "\n",
    "        prev_utterance = data['prev_utterance']\n",
    "        prev_utt_lengths = data['prev_length']\n",
    "\n",
    "        visual_context = data['concat_context']\n",
    "        target_img_feats = data['target_img_feats']\n",
    "\n",
    "        max_length_tensor = prev_utterance.shape[1]\n",
    "\n",
    "        masks = mask_attn(prev_utt_lengths, max_length_tensor, device)\n",
    "\n",
    "        visual_context_hid = model.relu(model.lin_viscontext(visual_context))\n",
    "        target_img_hid = model.relu(model.linear_separate(target_img_feats))\n",
    "\n",
    "        concat_visual_input = model.relu(model.linear_hid(torch.cat((visual_context_hid, target_img_hid), dim=1)))\n",
    "\n",
    "        embeds_words = model.embedding(prev_utterance)  # b, l, d\n",
    "\n",
    "        # pack sequence\n",
    "\n",
    "        sorted_prev_utt_lens, sorted_idx = torch.sort(prev_utt_lengths, descending=True)\n",
    "        embeds_words = embeds_words[sorted_idx]\n",
    "\n",
    "        concat_visual_input = concat_visual_input[sorted_idx]\n",
    "\n",
    "        # RuntimeError: Cannot pack empty tensors.\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(embeds_words, sorted_prev_utt_lens, batch_first=True)\n",
    "\n",
    "        # start lstm with average visual context:\n",
    "        # conditioned on the visual context\n",
    "\n",
    "        # he, ce = self.init_hidden(batch_size, device)\n",
    "        concat_visual_input = torch.stack((concat_visual_input, concat_visual_input), dim=0)\n",
    "\n",
    "        packed_outputs, hidden = model.lstm_encoder(packed_input, hx=(concat_visual_input, concat_visual_input))\n",
    "\n",
    "        # re-pad sequence\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        # already concat forward backward\n",
    "\n",
    "        # un-sort\n",
    "        _, reversed_idx = torch.sort(sorted_idx)\n",
    "        outputs = outputs[reversed_idx]\n",
    "\n",
    "        # ONLY THE HIDDEN AND OUTPUT ARE REVERSED\n",
    "        # next_utterance is aligned (pre_utterance info is not)\n",
    "        batch_out_hidden = hidden[0][:, reversed_idx]  # .squeeze(0)\n",
    "\n",
    "        # start decoder with these\n",
    "\n",
    "        # teacher forcing\n",
    "\n",
    "        decoder_hid = model.linear_dec(torch.cat((batch_out_hidden[0], batch_out_hidden[1]), dim=1))\n",
    "\n",
    "        history_att = model.lin2att_hist(outputs)\n",
    "\n",
    "        decoder_hid = decoder_hid.expand(beam_k, -1)\n",
    "\n",
    "        # multiple copies of the decoder\n",
    "        h1, c1 = decoder_hid, decoder_hid\n",
    "\n",
    "        # ***** beam search *****\n",
    "\n",
    "        gen_len = 0\n",
    "\n",
    "        decoder_input = sos_token.expand(beam_k, 1)  # beam_k sos copies\n",
    "\n",
    "        gen_sentences_k = decoder_input  # all start off with sos now\n",
    "\n",
    "        top_scores = torch.zeros(beam_k, 1).to(device)  # top-k generation scores\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # EOS?\n",
    "\n",
    "            if gen_len > max_len:\n",
    "                break  # very long sentence generated\n",
    "\n",
    "            # generate\n",
    "\n",
    "            # sos segment eos\n",
    "            # base model with visual input\n",
    "\n",
    "            decoder_embeds = model.embedding(decoder_input).squeeze(1)\n",
    "\n",
    "            h1, c1 = model.lstm_decoder(decoder_embeds, hx=(h1, c1))\n",
    "\n",
    "            h1_att = model.lin2att_hid(h1)\n",
    "\n",
    "            attention_out = model.attention(model.tanh(history_att + h1_att.unsqueeze(1)))\n",
    "\n",
    "            attention_out = attention_out.masked_fill_(masks, float('-inf'))\n",
    "\n",
    "            att_weights = model.softmax(attention_out)\n",
    "\n",
    "            att_context_vector = (history_att * att_weights).sum(dim=1)\n",
    "            \n",
    "            word_pred = F.log_softmax(model.lin2voc(torch.cat((h1, att_context_vector), dim=1)), dim=1)\n",
    "\n",
    "            word_pred = top_scores.expand_as(word_pred) + word_pred\n",
    "\n",
    "\n",
    "            if gen_len == 0:\n",
    "                # all same\n",
    "\n",
    "                # static std::tuple<Tensor, Tensor> at::topk(const Tensor &self, int64_t k,\n",
    "                # int64_t dim = -1, bool largest = true, bool sorted = true)\n",
    "\n",
    "                top_scores, top_words = word_pred[0].topk(beam_k, 0, True, True)\n",
    "\n",
    "            else:\n",
    "                # unrolled\n",
    "                top_scores, top_words = word_pred.view(-1).topk(beam_k, 0, True, True)\n",
    "\n",
    "            # vocab - 1 to exclude <NOHS>\n",
    "            sentence_index = top_words / (len(vocab)-1)  # which sentence it will be added to\n",
    "            word_index = top_words % (len(vocab)-1)  # predicted word\n",
    "\n",
    "            gen_len += 1\n",
    "\n",
    "            # add the newly generated word to the sentences\n",
    "            gen_sentences_k = torch.cat((gen_sentences_k[sentence_index], word_index.unsqueeze(1)), dim=1)\n",
    "\n",
    "            # there could be incomplete sentences\n",
    "            incomplete_sents_inds = [inc for inc in range(len(gen_sentences_k)) if\n",
    "                                     eos_token not in gen_sentences_k[inc]]\n",
    "\n",
    "            complete_sents_inds = list(set(range(len(word_index))) - set(incomplete_sents_inds))\n",
    "\n",
    "            # save the completed sentences\n",
    "            if len(complete_sents_inds) > 0:\n",
    "                completed_sentences.extend(gen_sentences_k[complete_sents_inds].tolist())\n",
    "                completed_scores.extend(top_scores[complete_sents_inds])\n",
    "\n",
    "                beam_k -= len(complete_sents_inds)  # fewer, because we closed at least 1 beam\n",
    "\n",
    "            if beam_k == 0:\n",
    "                break\n",
    "\n",
    "            # continue generation for the incomplete sentences\n",
    "            gen_sentences_k = gen_sentences_k[incomplete_sents_inds]\n",
    "\n",
    "            # use the ongoing hidden states of the incomplete sentences\n",
    "            h1, c1 = h1[sentence_index[incomplete_sents_inds]], c1[sentence_index[incomplete_sents_inds]],\n",
    "\n",
    "            top_scores = top_scores[incomplete_sents_inds].unsqueeze(1)\n",
    "            decoder_input = word_index[incomplete_sents_inds]\n",
    "            decoder_hid = decoder_hid[incomplete_sents_inds]\n",
    "\n",
    "        if len(completed_scores) == 0:\n",
    "\n",
    "            empty_count += 1\n",
    "            #print('emptyseq', empty_count)\n",
    "\n",
    "            # all incomplete here\n",
    "\n",
    "            completed_sentences.extend((gen_sentences_k[incomplete_sents_inds].tolist()))\n",
    "            completed_scores.extend(top_scores[incomplete_sents_inds])\n",
    "\n",
    "        sorted_scores, sorted_indices = torch.sort(torch.tensor(completed_scores), descending=True)\n",
    "\n",
    "        best_seq = completed_sentences[sorted_indices[0]]\n",
    "\n",
    "        hypothesis = [vocab.index2word[w] for w in best_seq if w not in\n",
    "                      [vocab.word2index['<sos>'], vocab.word2index['<eos>'], vocab.word2index['<pad>']]]\n",
    "        # remove sos and pads # I want to check eos\n",
    "        hypothesis_string = ' '.join(hypothesis)\n",
    "        hypotheses.append(hypothesis_string)\n",
    "\n",
    "        if not os.path.isfile('speaker_outputs/refs_' + file_name + '.json'):\n",
    "            # Reference\n",
    "            references.append(reference_chain)\n",
    "\n",
    "        if print_gen:\n",
    "            # Reference\n",
    "            print('REF:', orig_text_reference) # single one\n",
    "            print('REF chain:', reference_chain) \n",
    "            print('HYP:', hypothesis_string, '\\n')\n",
    "\n",
    "    if os.path.isfile('speaker_outputs/refs_' + file_name + '.json'):\n",
    "        with open('speaker_outputs/refs_' + file_name + '.json', 'r') as f:\n",
    "            references = json.load(f)\n",
    "    else:\n",
    "        with open('speaker_outputs/refs_' + file_name + '.json', 'w') as f:\n",
    "            json.dump(references, f)\n",
    "            \n",
    "    # Calculate scores\n",
    "    metrics_dict = nlgeval_obj.compute_metrics(references, hypotheses)\n",
    "    print(metrics_dict)\n",
    "\n",
    "    (P, R, Fs), hashname = score(hypotheses, references, lang='en', return_hash=True, model_type=\"bert-base-uncased\")\n",
    "    print(f'{hashname}: P={P.mean().item():.6f} R={R.mean().item():.6f} F={Fs.mean().item():.6f}')\n",
    "\n",
    "    if args.metric == 'cider':\n",
    "        selected_metric_score = metrics_dict['CIDEr']\n",
    "        print(round(selected_metric_score, 5))\n",
    "\n",
    "    elif args.metric == 'bert':\n",
    "        selected_metric_score = Fs.mean().item()\n",
    "        print(round(selected_metric_score, 5))\n",
    "\n",
    "    # from https://github.com/Maluuba/nlg-eval\n",
    "    # where references is a list of lists of ground truth reference text strings and hypothesis is a list of\n",
    "    # hypothesis text strings. Each inner list in references is one set of references for the hypothesis\n",
    "    # (a list of single reference strings for each sentence in hypothesis in the same order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    nlge = NLGEval(no_skipthoughts=True, no_glove=True)\n",
    "\n",
    "    speaker_files = ['saved_models/model_speaker_hist_att_42_bert_2020-05-21-15-13-22.pkl']\n",
    "#     ['saved_models/model_speaker_hist_att_1_bert_2020-05-22-16-40-11.pkl',\n",
    "# 'saved_models/model_speaker_hist_att_2_bert_2020-05-22-16-41-12.pkl',\n",
    "# 'saved_models/model_speaker_hist_att_3_bert_2020-05-22-16-42-13.pkl',\n",
    "# 'saved_models/model_speaker_hist_att_4_bert_2020-05-22-16-43-13.pkl']\n",
    "\n",
    "    for speaker_file in speaker_files:\n",
    "\n",
    "        seed = 28\n",
    "\n",
    "        # for reproducibility\n",
    "        print(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        print(speaker_file)\n",
    "\n",
    "        checkpoint = torch.load(speaker_file, map_location=device)\n",
    "\n",
    "        args = checkpoint['args']\n",
    "\n",
    "        model_type = args.model_type\n",
    "\n",
    "        print(\"Loading the vocab...\")\n",
    "        vocab = Vocab(os.path.join(args.data_path, args.vocab_file))\n",
    "        vocab.index2word[len(vocab)] = '<nohs>'  # special token placeholder for no prev utt\n",
    "        vocab.word2index['<nohs>'] = len(vocab)  # len(vocab) updated (depends on w2i)\n",
    "\n",
    "        testset = SpeakerDataset(\n",
    "            data_dir=args.data_path,\n",
    "            utterances_file='test_' + args.utterances_file,\n",
    "            vectors_file=args.vectors_file,\n",
    "            chain_file='test_' + args.chains_file,\n",
    "            orig_ref_file='test_' + args.orig_ref_file,\n",
    "            split='test',\n",
    "            subset_size=1 #args.subset_size\n",
    "        )\n",
    "\n",
    "        print('vocab len', len(vocab))\n",
    "        print('test len', len(testset), 'longest sentence', testset.max_len)\n",
    "\n",
    "        max_len = 30  # for beam search\n",
    "\n",
    "        img_dim = 2048\n",
    "\n",
    "        embedding_dim = args.embedding_dim\n",
    "        hidden_dim = args.hidden_dim\n",
    "        att_dim = args.attention_dim\n",
    "\n",
    "        dropout_prob = args.dropout_prob\n",
    "        beam_size = args.beam_size\n",
    "\n",
    "        metric = args.metric\n",
    "\n",
    "        shuffle = args.shuffle\n",
    "        normalize = args.normalize\n",
    "        breaking = args.breaking\n",
    "\n",
    "        print_gen = args.print\n",
    "\n",
    "        # depending on the selected model type, we will have a different architecture\n",
    "\n",
    "        model = SpeakerModelHistAtt(len(vocab), embedding_dim, hidden_dim, img_dim, dropout_prob, att_dim).to(device)\n",
    "\n",
    "        batch_size = 1\n",
    "\n",
    "        load_params_test = {'batch_size': 1, 'shuffle': False,\n",
    "                            'collate_fn': SpeakerDataset.get_collate_fn(device, vocab['<sos>'], vocab['<eos>'],\n",
    "                                                                        vocab['<nohs>'])}\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(testset, **load_params_test)\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model = model.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            isValidation = False\n",
    "            isTest = True\n",
    "            print('\\nTest Eval')\n",
    "\n",
    "            # THIS IS test EVAL_BEAM\n",
    "            print('beam')\n",
    "\n",
    "            # best_score and timestamp not so necessary here\n",
    "            best_score = checkpoint['accuracy']  # cider or bert\n",
    "            t = datetime.datetime.now()\n",
    "            timestamp = str(t.date()) + '-' + str(t.hour) + '-' + str(t.minute) + '-' + str(t.second)\n",
    "\n",
    "            print_gen = True\n",
    "            \n",
    "            # USING THE DEMO METHOD PROVIDED ABOVE\n",
    "            eval_beam_histatt_DEMO(test_loader, model, args, best_score, print_gen, device,\n",
    "                                      beam_size, max_len, vocab, mask_attn, nlge, isValidation, timestamp, isTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
